---
title: "DarkBERT: A Language Model for the Dark Side of the Internet"
author: JOSEOYEON
date: 2023-07-20 10:39:30
categories: [Algorithm, AI, NLP]
tags: [DarkBERT]
---


# DarkBERT - A Language Model for the Dark Side of the Internet

학회 - ACL 2023 Main Conference

<br/>

## DarkBERT 기능

다크버트의 활용도는 다음 4가지 영역에서 의미가 있습니다.<br/>
다크웹 페이지 분류와 렌섬웨어 유출 사이트 탐지, 그리고 불법 활동 감지, 은어와 같은 위협 키워드를 추론하는데 있어서 좋은 성능을 보인다고 합니다.<br/>
학습된 내용이 범죄와 관련된 민감한 내용인 만큼 공개하지는 않았지만, S2W 인텔리전스 솔루션 자비스와 퀘이사에도 내장되어 있다고 합니다. <br/>

### 1) 다크웹 페이지 카테고리 별 분류
- 음란물, 해킹, 폭력 등으로 분류하는 다크웹 페이지 분류 
### 2) 랜섬웨어 유출 사이트 탐지
- 기밀 데이터 유출 사이트 자동으로 감지 (랜섬웨어 그룹 정보 수집에 사용)
### 3) 주목할만한 내용 추출 및 감지
- 다크웹의 불법 활동 관련 정보 공유/판매 플랫폼 게시물을 필터링하여 주목할만한 스레드 자동 감지 (예: 기밀 정보 판매/공유 또는 악의적인 해킹 도구) 
### 4) 위협 키워드 추론
- 다크웹에서 완전히 다른 의미로 사용되는 범죄 은어 훈련, 다크웹 컨텐츠 문맥 이해 지원

## 논문 리뷰 

```Dark Web```
온라인 웹 사이트에 접속한 개인 식별 및 추적을 방지하는 것이 목표 
- 데이터 유출, 마약 판매 등 불법적인 컨텐츠 유통, 범죄의 온상지
네트워크 트래픽의 출발지와 목적지를 난독화하는 암호화 및 라우팅 기술 사용 
- 접속을 위해 Tor와 같은 특수 오버레이 네트워크 애플리케이션 필요 
- 클라이언트와 서버 IP 주소가 서로 숨겨져 있는 hidden 서비스(.onion 서비스)를 호스팅

### 다크웹에 적합한 NLP 도구의 필요성

다크웹에서의 사이버 위협 지표(IOC)를 추출하고 분석하기 위해 NLP(Natural Language Processing) 기법을 활용하는 것은 현대 사이버 보안 분야에서 매우 중요한 주제입니다. NLP를 활용하여 다크웹의 언어적 특성을 이해하고 유용한 정보를 추출하는 솔루션을 개발하는 것은 CTI(Cyber Threat Intelligence)에 필수적입니다. <br/>

BERT(Bidirectional Encoder Representations from Transformers)와 같은 사전 학습된 언어 모델들은 표면 웹 콘텐츠(위키백과 등)에 대해 탁월한 성능을 보여주었습니다. 하지만 다크웹은 일반적인 표면 웹과는 다른 언어적 특성을 가지기 때문에, BERT와 같은 모델은 다크웹에서의 정보 추출에 적합하지 않을 수 있습니다. 따라서 다크웹에서 유용한 정보를 추출하기 위해서는 다크웹에 특화된 NLP 도구가 필요합니다. <br/>

이러한 요구에 맞게 다크웹에서 사용 가능하고 최첨단 성능을 달성할 수 있는 NLP 방법을 탐구해야 합니다. 다크웹은 표면 웹과는 다른 언어적 특성과 독특한 문체를 가지고 있으므로, 이러한 특성을 잘 반영하고 이해할 수 있는 NLP 기법을 개발하는 것이 중요합니다.<br/>

다크웹에서 효율적이고 신속한 위협 지표(IOC) 획득을 위해 NLP를 활용하는 것은 CTI 분야에서 가치 있는 연구 주제 중 하나입니다. 이를 통해 다크웹 상에서의 사이버 위협을 조기에 감지하고 대응하는 데 도움이 될 수 있습니다.<br/>

## DarkBERT Construction 

![image](https://github.com/cotes2020/jekyll-theme-chirpy/assets/46625602/aa1479ab-83ea-4d8a-9925-f1c6241e7beb)


- 데이터 셋 수집 - 다크웹 페이지에서 텍스트 말뭉치 (corpus)를 구축
- 데이터 전처리 - 원시 텍스트 말뭉치를 필터링하여 불필요한 정보와 악성 컨텐츠 제거
- 학습- 텍스트 말뭉치를 사용하여 DarkBERT 사전 훈련


수집한 대규모 텍스트 말뭉치를 분석하기 위한 방법으로 공개 저장소에서 seed 주소(.onion)를 수집하고, 다크웹 초기 seed 주소 페이지를 탐색하여 참조하는 페이지로 확장한 후, 각 페이지의 HTML을 획득하여 제목과 본문 요소로 페이지를 구문 분석하는 방법을 사용하고 있습니다. 이러한 방법으로 수집된 페이지들은 다크웹의 콘텐츠를 대표하는 데이터들이 될 수 있습니다.

수집한 페이지를 분류하기 위해 fastText와 같은 언어 모델을 사용하고, 각 페이지를 기본 언어로 분류한 뒤, 영어 페이지만을 선별한 것으로 보입니다. 이는 영어가 다크웹 콘텐츠의 대다수를 차지하고 있기 때문에 영어 페이지만 선별하여 분석하고자 하는 목적이 있을 수 있습니다.

위의 방법을 통해 총 610만 개의 페이지가 수집되었습니다. 이러한 대규모의 텍스트 말뭉치를 가지고 다양한 NLP 기법을 적용하여 다크웹에서의 정보를 추출하고, 위협 지표(IOC)를 탐지하는 등의 연구에 활용할 수 있을 것입니다.

하지만, 다크웹에서의 데이터 수집은 민감한 주제이며, 법적인 제약 사항이 따르거나 개인 정보 보호와 관련하여 주의가 필요합니다. 또한 다크웹은 불법적이거나 위험한 콘텐츠를 포함하고 있을 수 있으므로, 이러한 데이터를 다룰 때는 윤리적인 책임과 신중함을 가지고 접근해야 합니다. 합법적이고 윤리적인 방법으로 데이터 수집 및 분석이 이루어지도록 주의해야 합니다.


![image](https://github.com/cotes2020/jekyll-theme-chirpy/assets/46625602/2fc41af2-b440-4bb8-a71e-deb9319ef9be)
![image](https://github.com/cotes2020/jekyll-theme-chirpy/assets/46625602/c3b74154-808a-4967-9fcb-0348f43f8d7a)


### Data Filtering and Text Processing – 무의미하거나 민감한 데이터 전처리

1. 의미없는 페이지 전처리
- 비정상적으로 많은 문자수나 적은 문자수를 가진 페이지 제거 
    - 정보 밀도가 낮고, 오류 메시지나 다른 페이지의 중복이 포함된 페이지들을 걸러냄
- 카테고리 별 데이터 셋 조정 및 중복 제거의 조치
    - 데이터 셋을 정제하여 사전 훈련된 언어 모델의 성능을 향상시키기 위한 작업 수행
2. 민감한 데이터 처리 
- 언어 모델은 민감한 데이터로 사전 훈련된 후에 처리하여 민감한 정보를 추출할 수 없도록 할 수 있지만 보다 정교한 공격을 당할 가능성 존재. 
- 민감한 데이터를 포함한 페이지들은 사전 훈련 전에 민감한 정보를 마스킹을 하거나 제거하여 사전 처리

### 의미없는 페이지 제거를 위한 문자 수 필터링
- 비정상적으로 낮은 문자 수를 포함하는 페이지
    - "404를 찾을 수 없음", "캡처 오류"와 같은 오류 메시지, "로그인 요청", "이미 계정이 있습니까?"와 같은 로그인 메시지 등의 정보가 포함된 페이지들
    - 다크웹 콘텐츠의 낮은 정보 밀도를 반영하여 매우 유용하지 않음

- 비정상적으로 높은 문자 수를 포함하는 페이지
    - 대부분 키워드 목록이거나 특정 문자열의 연속적인 반복 	
    - 다크웹 콘텐츠의 낮은 정보 밀도를 포함하기 때문에 매우 유용하지 않음


### 카테고리 별 학습 데이터 균형 맞춤
- Dark Web 페이지 10개 범주 자동 분류
    - CoDA(데이터 셋)에서 10개의 범주 중 9개를 사용, 정확한 분류를 위해 기타 범주 제외
    - 기타 범주의 대부분의 페이지(로그인 페이지, 오류 페이지 등)는 이미 문자 수 필터링을 통해 제거
    - 포르노가 가장 많은 41.7% 차지. 도박/무기 같은 카테고리는 각각 전체 페이지의 1% 미만
- 범주별 데이터 셋 균형 맞춤
    - 데이터 셋이 많은 범주에서 페이지 무작위로 제거하여 균형 맞춤
    - 중복제거로 인한 데이터 크기 감소, 대부분의 범주에서 중복제거율이 10% 미만
    - 도박 및 무기/무기 범주- 데이터 크기 작아 제거 안함(중복제거율과 총 감소율은 동일)
    - 가장 작은 범주(무기/무기)와 가장 큰 범주(포르노) 크기 차이 70배  7배(최종 말뭉치)

![image](https://github.com/cotes2020/jekyll-theme-chirpy/assets/46625602/128d1a6d-840f-4542-a487-df2552e2f6f6)

### Data Text Processing Details - Identifier Mask Details

텍스트 전처리 - 식별자 마스크(정규표현식) 적용
- 구현- 고유한 패턴이 포함된 URL 및 IP 주소는 정규식 사용 제거 
- 전자 메일 주소 마스킹- Dark Web에서 통신 수단(ProtonMail) 단일 개인을 추적할 수 있는 문자열 포함 가능성 높음
- URL- 다크웹의 표현으로 학습되지 않도록 모든 URL을 마스킹
- IP 주소-다크웹은 IP 주소를 숨기는 데 사용되지만 일부 페이지는 텍스트에 IP 주소를 포함할 수 있음
    -IP 주소가 포함된 페이지 대부분은 Tor 릴레이 사이트를 나타냄

![image](https://github.com/cotes2020/jekyll-theme-chirpy/assets/46625602/2c82f8aa-e9a7-4c75-ae39-a305715da0dd)

## DarkBERT Pretraining - RoBERTa 기반 Dark Web 사전 훈련 모델 (DarkBERT)

- 사전 교육을 위해 기존 모델 아키텍처 활용
    - 계산 부하를 줄이고 기존 모델에서 학습한 일반적인 영어 표현을 유지하기 위함
- 다음 문장 예측(NSP) 작업을 하지 않기 때문에 RoBERTa를 기본 모델로 선택
    - 다크웹 말뭉치는 표면 웹에 비해 널리 보급되지 않은 언어구조를 사용하므로 RoBERTa의 기반 모델로 적합
- RoBERTa와 DarkBERT 호환성
    - RoBERTa와 DarkBERT는 동일한 BPE 토큰화 어휘를 사용하여 호환성을 유지
    - 사전 훈련 말뭉치의 각 페이지는 RoBERTa의 구분자 토큰 </s>를 사용하여 구분
    - Dark Web 데이터를 RoBERTa 기반으로 사전 훈련하고, 이를 DarkBERT로 활용

![image](https://github.com/cotes2020/jekyll-theme-chirpy/assets/46625602/6e6d04e1-ef26-4012-8d68-628ed8e975a4)

## BPE 용어 설명

- BPE(Byte-Pair Encoding)- 서브워드 분리(subword segmentation) 알고리즘
    - 글자(character) 단위에서 점차적으로 단어 집합(vocabulary)을 만들어 내는 bottom-up 방식 사용 학습
    - BPE 기반 토큰화 기법은 분석 대상 언어에 대한 지식 필요 없음 


![image](https://github.com/cotes2020/jekyll-theme-chirpy/assets/46625602/56d03046-76a1-4386-9352-471f0e55828b)

### 실험 환경 및 DarkBERT 학습 데이터
- 두 버전의 DarkBERT
    1) 원시 텍스트 데이터(공백 제거(white space)가 적용됨) 사용
    2) 사전 처리된 텍스트 사용
- 시스템- Intel Xeon Gold 6348 CPU @ 2.60GHz 및 4개의 NVIDIA A100 80GB GPU
- 시간 - 각 버전의 DarkBERT를 실행하는 데 약 15일 걸림
- GPU 4개가 모두 사전 교육 프로세스를 실행하는 데 사용
- 두 버전의 DarkBERT 약 20K 단계에서 훈련 손실(Training Loss)이 감소하는 것을 멈춤
- 20,000 단계에서 저장된 모델을 평가에 사용
- PyTorch로 작성된 스크립트를 사용하여 사전 훈련 수행

![image](https://github.com/cotes2020/jekyll-theme-chirpy/assets/46625602/de1313f7-aa7a-475a-bd73-29ef92f4c9ad)

## Evaluation- Dark Web Activity Classification 

평가 데이터 셋 - DUTA, CoDA
비교 알고리즘 - DarkBERT, BERT, RoBERTa


### 다크웹 데이터 세트 활용 - DUTA 및 CoDA

다크웹 데이터 세트 소개
최신 버전의 DUTA(DUTA-10K)와 CoDA의 다크웹 영문 텍스트 사용
DUTA와 CoDA는 서로 다른 분류 방법을 사용하므로 각 데이터 세트에 대해 별도의 분류자로 교육
DUTA는 크기가 매우 작은 특정 범주가 포함됨(예- 인신매매 범주 아래에 3페이지만 있음) 
 페이지 수가 낮은 범주는 제거 (전체 페이지 수의 1% 미만) 
CoDA 데이터 세트는 DUTA와 달리 수정 없이 사용

DUTA -Classifying Illegal Activities on Tor Network Based on Web Textual Contents (2017)
CODA -Shedding New Light on the Language of the 다크웹 (2022)

![image](https://github.com/joseoyeon/20221109_KDFS/assets/46625602/619ad17a-d624-4d79-b1bf-d7eca7e07d1b)

### 분류 실험 - DarkBERT, BERT, RoBERTa

- DarkBERT의 두 가지 버전과 언어 모델인 BERT와 RoBERTa에서 수행

- RoBERTa
    - RoBERTa는 대문자로 표시된 단어와 대문자로 표시되지 않은 단어를 구별할 수 있는 언어 모델이지만 BERT는 case 모델(대소문자 구분)과 미 case 모델(대소문자 미구분)의 두 가지 버전으로 제공
    - 문자 대소문자가 분류 성능에 영향을 미치는지 관찰하기 위해 모든 문자가 소문자로 변환되는 별도 버전의 DUTA 및 CoDA 구축

- 알고리즘 
    - DarkBERT(원시 및 전처리) 두 버전 / BERT(대소구분 및 대소미구분) 두 버전 / RoBERTa, 총 5개
- 데이터 셋
    - 대소문자 구분/미구분 DUTA 및 CoDA 두 버전을 사용하여 다크웹 활동 분류 작업을 평가

- CoDA 데이터 세트에서 언어 모델들이 뛰어난 성능을 보여줌
    - DUTA에 포함된 일부 범주는 분류 작업에 적합하지 않을 수 있음 
    - Hosting & Software 범주는 중복된 텍스트 포함, 모델에 과적합
    - 일부 페이지는 DUTA 데이터 세트 분류 측면에서 모호
- DarkBERT 모델은 모든 종류의 데이터셋에서 가장 높은 분류 정확도를 보임

![image](https://github.com/joseoyeon/20221109_KDFS/assets/46625602/16db59f3-0b33-4b85-ac86-0192538deb4e)

## DarkBERT vs. 다른 언어 모델 - 성능 비교

- DarkBERT의 우수성
    - DarkBERT가 데이터 세트와 그 변형 모두에서 다른 언어 모델을 능가한다는 것을 관찰
    - 마약, 전자 제품 및 도박과 같은 일부 범주는 네 가지 모델 모두에서 매우 유사한 성능을 보여줌 
    - 해당 범주의 페이지 유사성이 높거나, 다크웹 언어의 차이가 있어도 쉬운 분류 데이터 셋일 수 있음


![image](https://github.com/joseoyeon/20221109_KDFS/assets/46625602/8532365c-f389-443f-a09c-b5bbd11cba0e)
![image](https://github.com/joseoyeon/20221109_KDFS/assets/46625602/c30a920e-3334-4da2-8456-d71f164e561e)

- 오류 분석 결과
    - BERT와 RoBERTa에서 잘못 분류, DarkBERT에 의해 올바르게 분류된 대부분의 페이지특징
    - 다크웹의 특정 활동에서 볼 수 있는 많은 도메인별 용어 또는 주요 구문이 포함됨 
- 신용카드 판매자 서비스 이름이 포함된 페이지는 BERT와 RoBERTa에서 기타로 잘못 구별 
- 포르노 페이지의 "red moon" 문구가 BERT와 RoBERTa에서 모두 기타로 잘못 분류됨
- 블록체인 및 암호화폐 용어를 포함된 것은 BERT와 RoBERTa에서 기타로 잘못 분류

- DarkBERT의 우월성
    - SurfaceWeb에서는 일반적으로 사용되지 않는 문구를 포함하는 페이지를 정확하게 분류
    - BERT와 RoBERTa는 다크웹 단어를 일반적인 단어로 간주하기 때문에 기타 범주로 잘못 분류


## Use Cases in the Cybersecurity Domain

- Ransomware Leak Site Detection
- Noteworthy Thread Detection
- Threat Keyword Inference

---

## Ransomware Leak Site Detection - 이진 분류 문제 풀이

- 이진 분류 문제로 개인정보누출 사이트 여부 예측
4개의 인기 랜섬웨어 그룹의 유출 사이트를 2년간 모니터링 중
- 유출 사이트에는 피해자 조직명, 유출된 데이터에 대한 설명, 샘플 데이터를 포함한 위협 정보 포함됨
- 유출 관련 페이지를 모으기 위해 자동 분류 모델 사용 =>  해당 연구는 유출 사이트가 대부분 자동 분류에 의해 해킹으로 분류되고 그 다음으로 암호화폐, 금융, 기타로 분류된다는 것을 시사

![image](https://github.com/joseoyeon/20221109_KDFS/assets/46625602/0bf2ea31-41fc-4d37-8751-bf5d8dbe0ff5)
![image](https://github.com/joseoyeon/20221109_KDFS/assets/46625602/32a3c468-6e77-464c-b247-893945714c91)

##  Noteworthy Thread Detection - 이진 분류 문제 풀이

- 페이지에 위협정보가 포함되어 있는지 이진 분류 문제로 예측
    - 페이지를 주목할 만한 것으로 식별하는 것은 매우 주관적인 작업
    - 데이터 세트의 품질 보장을 위해 다크웹 사이버 위협 인텔리전스 회사의 연구원 2명을 모집하여 위협 정보 포함 페이지 태깅

- 해킹 포럼이 다음 활동 중 하나를 설명하는 경우 주목할 만하다고 태깅
1. 회사 기밀 자산의 유포 - 관리자 접속, 직원/고객 정보, 트랜잭션, 코드 및 기밀 문서
2. 개인 정보 유포 - 신용정보, 의료기록, 정치참여, 여권, 신분증, 시민권 등 
3. 악성코드나 취약점의 유포 - 대중적인 소프트웨어나 조직을 대상으로, 특히 대기업, 공공기관, 산업체를 대상으로 한 활동에 중점

##  Noteworthy Thread Detection - 이진 분류 문제 풀이

- 이진 분류 문제로 포럼 스레드가 주목할 만한지 여부 예측
    - 그림 B - DarkBERT에 의해 올바르게 분류되었지만 다른 모델에 의해 잘못 분류된 주목할 만한 스레드 샘플

![image](https://github.com/joseoyeon/20221109_KDFS/assets/46625602/580b676c-23bc-4173-9f32-6cbfe9560c74)

## Threat Keyword Inference – 적합 단어 찾기

- Keyword Inference- 문장의 마스크된 위치에 가장 적합한 단어 찾기 
- 다크웹에서 위협 및 약물 판매와 의미론적으로 관련된 키워드 도출을 위한 평가 
- 약물을 주제로 하는 말뭉치에서 DarkBERT와 BERT(Reddit) 비교
- 마약을 광고하는 다크웹의 마약 판매 페이지 =>  제목을 마스크하고 DarkBERT와 BERT(Reddit)로 가장 의미론적으로 관련된 단어를 제안한 결과

![image](https://github.com/joseoyeon/20221109_KDFS/assets/46625602/34fda039-9292-4a24-a140-d33bb9509352)

- 결과 및 토론
    - 측정 결과 - DarkBERT(CoDA)는 k=10, 20에서 BERT(Reddit)를 능가,  더 높은 k 값에서는 추월됨
    - 서페이스 웹 데이터 세트는 주로 서피스 웹에서 파생된 완곡한 표현이 포함되어 있으며, 의미론적으로 다크웹 관련 단어는 데이터 세트에 포함되어 있지 않음

- 예시 - 다크웹에서만 자주 볼 수 있는 약물 이름(Tesla와 Champagne)과 달리 크리스탈과 얼음은 서피스 웹과 다크웹에서 모두 사용되기 때문에 DarkBERT(CoDA)와 BERT(Reddit)에 의해 감지됨

![image](https://github.com/joseoyeon/20221109_KDFS/assets/46625602/2aeeb23f-3b92-4c7b-ab72-51a9ae7b8fa0)

## Conclusion

- RoBERTa 기반 다크웹 텍스트 데이터를 학습한 언어 모델 DarkBERT 제안
- 데이터 수집 - 대규모 다크웹 말뭉치 수집 하여 모델을 사전 훈련
- 데이터 전처리 - 민감한 정보를 배제하기 위해 데이터 필터링 및 중복제거를 수행하여 사전 훈련 말뭉치 정제 
- 사전 훈련 - 전처리된 말뭉치를 이용하여 RoBERTa 아키텍처 기반으로 사전 훈련 
- 평가 및 성능 확인 - DarkBERT는 다크웹 도메인 작업에 대한 평가를 수행하여 기존 언어 모델을 능가하는 성능을 보임

## CoDA 데이터 셋을 활용한 실습
RoBERTa를 이용한 학습 및 분류 실습

![image](https://github.com/joseoyeon/20221109_KDFS/assets/46625602/1cdd4785-04c4-4547-871d-62ceface1fa2)

- 제공 받은 CoDA 데이터 셋
- 10000개의 라벨링 된 다크웹 텍스트 
- 민감 데이터들은 마스킹 처리됨 

## RoBERTa Encoder 입력 형태
- Input_ids - BPE 알고리즘에 따라 subword 단위로 분리된 token id 리스트
- token_type_ids - 각 token 이 어떤 문장에 속하는지 나타내는 리스트
- attention_mask - attention 연산이 수행되어야 할 token과 무시해야 할 token을 구별하는 정보가 담긴 리스트
![image](https://github.com/joseoyeon/20221109_KDFS/assets/46625602/2830c95a-7683-4021-a93c-38bffe929a9a)

## RoBERTa 기본 하이퍼 파라미터로 학습
- Dropout Layer- 네트워크 과적합 방지를 위한 레이어
    - 무작위로 뉴런 삭제
- Flatten Layer - Feature 추출 계층과 호환을 위한 레이어
    - 2D => 1D 변환 
- Dense - 분류를 위한 학습 레이어(softmax 활성화 함수 존재 최종 레이어)

![image](https://github.com/joseoyeon/20221109_KDFS/assets/46625602/f2e64088-411b-4997-a70c-c2f3c0f0b733)

![image](https://github.com/joseoyeon/20221109_KDFS/assets/46625602/a835b529-ef22-4ed1-ae2b-407d2522b9ac)


---

**Refference**

* Jin, Youngjin, et al. "DarkBERT- A Language Model for the Dark Side of the Internet." arXiv preprint arXiv:2305.08596 (2023).
[https://arxiv.org/abs/2305.08596](https://arxiv.org/abs/2305.08596)

